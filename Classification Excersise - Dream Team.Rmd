---
title: "Regression"
author: "Ran Dou, Mduduzi Langwenya, Kimo Li, Siyan Lin, Muhammad Furqan Shaikh, Tianyi Zhou"
date: "03/09/2019"
output: html_document
---

### Load the packages
```{r, message=FALSE, warning=FALSE}
rm(list=ls())
library(tidyverse)
library(readr)
library(tidyverse)
library(forecast)
library(leaps)
library(pROC)
library(ggplot2)
library(reshape)
library(leaps)
library(corrplot)
library(knitr)
library(broom)
library(caret)
library(class)
library(e1071)
```

### I. Data cleaning and impution

##### Data importing
```{r, warning=FALSE, message=FALSE}
###import the raw diabetes data
diabetes <- read_csv("diabetes.csv")
###delete all the missing valuse
diabetes1 <- diabetes %>%
  filter( Glucose !=0 & BMI != 0 & BloodPressure != 0 & Insulin != 0 & SkinThickness != 0) %>%
  select(Glucose, Insulin, Outcome, BMI, SkinThickness )
```

##### Fill-in Zero Value
###### 1) Insulin
```{r,  message=FALSE}
### Insulin 
# stepwise for choosing models for Insulin 
insu.lm.null <- lm(Insulin~1, data = diabetes1)
insu.lm <- lm(Insulin~., data = diabetes1)
summary(insu.lm.null)
summary(insu.lm)
insu.lm.step_both <- step(insu.lm, direction = "both")
sum_both <- summary(insu.lm.step_both)
### create the model for imputing Insulin missing values
lm.data <- lm (Insulin ~ Glucose + BMI, data=diabetes1)
pred.1 <- predict (lm.data, diabetes1)
impute <-function(a, a.impute){
         ifelse(a$Insulin == 0, round(a.impute, 0), a$Insulin)
}
diabetes$newInsu <- impute(diabetes, pred.1)
```

###### 2) Skinthickness 
```{r}
### stepwise for choosing models for Insulin 
skin.lm.null <- lm(SkinThickness~1, data = diabetes1)
skin.lm <- lm(SkinThickness~., data = diabetes1)
skin.lm.step_both <- step(skin.lm, direction = "both")
sum_both_skin <- summary(skin.lm.step_both)
### create the model for imputing SkinThickness missing values
lm2.data <- lm(SkinThickness ~ BMI, data=diabetes1)
pred.2 <- predict (lm2.data, diabetes1)
impute <-function(a, a.impute){
  ifelse(a$SkinThickness == 0, round(a.impute, 0), a$SkinThickness)
}
diabetes$newSkin <- impute(diabetes, pred.2)
```

```{r}
################################ logistic regression part #############################
# CHANGE DATA TYPE
diabetes$Outcome <- as.factor(diabetes$Outcome)
diabetes$Pregnancies <- as.factor(diabetes$Pregnancies)

# divide data into train and test set
set.seed(1)
randOrder = order(runif(nrow(diabetes)))
train.df = subset(diabetes,randOrder < .8 * nrow(diabetes))
test.df = subset(diabetes,randOrder > .8 * nrow(diabetes))
```

##### correlation matrix

```{r, fig.width=6}
# plot the correlation matrix visual
corr.df <- train.df
corr.df$Outcome <- as.numeric(corr.df$Outcome)
corr.df$Pregnancies <- as.numeric(corr.df$Pregnancies)
cor <- cor(corr.df)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor, method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
```

```{r}
train.df$Outcome <- as.factor(train.df$Outcome)
train.df$Pregnancies <- as.factor(train.df$Pregnancies)
```

################################################################################################### 

# Logistic Regression

```{r}
### Forward Step-wise
# create model with no predictors for bottom of search range
dia.lm.null <- glm(Outcome~1, data = train.df, family = binomial)
dia.lm <- glm(Outcome~., data = train.df, family = binomial)
# use step() to run forward selection
dia.lm.step_for <- step(dia.lm.null,   
                    scope=list(lower=dia.lm.null, upper=dia.lm), direction = "forward")
sum_for <- summary(dia.lm.step_for) 

# Backward Step-wise
dia.lm.step_back <- step(dia.lm, direction = "backward")
sum_back <- summary(dia.lm.step_back) 

# Both Direction Step-wise
dia.lm.step_both <- step(dia.lm, direction = "both")
sum_both <- summary(dia.lm.step_both) 

# search
search <- regsubsets(Outcome ~ ., data = train.df, nbest = 1, 	nvmax = dim(train.df)[2], method = "exhaustive")
sum_sear <-summary(search)
sum_sear$which;
sum_sear$rsq;
sum_sear$adjr2;
sum_sear$Cp;

# comparison 
# same models with different methods
sum_for$coefficients
sum_back$coefficients
sum_both$coefficients

# best model with aic 536.4962
sum_for$aic

# Prediction on test data and accuracy test (73.1%)
tst_pred <- ifelse(predict(dia.lm.step_for, newdata = test.df, type = "response") > 0.5, "Yes", "No")
tst_tab <- table(predicted = tst_pred, actual = test.df$Outcome); sum(diag(tst_tab))/sum(tst_tab)
test_prob <- predict(dia.lm.step_for, newdata = test.df, type = "response")
test_roc <- roc(test.df$Outcome ~ test_prob, plot = TRUE, print.auc = TRUE) # 0.774
```

```{r, fig.width=3}
model1_data <- augment(dia.lm.step_for) %>% 
  mutate(index = 1:n()) %>%
  mutate(Outcome = ifelse(Outcome == "1", "0", "1"))
### Create theme for plots
theme <- theme_test(base_family = "Times New Roman") + theme(plot.title = element_text(hjust = 0.5), 
         legend.position = "bottom", panel.grid.minor = element_blank(), axis.ticks.x = element_blank(),
         axis.ticks.y = element_blank(), panel.grid.major = element_blank())
c <- ggplot(model1_data, aes(index, .std.resid, color = Outcome)) + 
  geom_point(stat = "identity") +
  labs(title = "Standardized Deviance Residuals", y = "Residual Std", x ="Residuals") +
  theme
c
```

################################################################################################### 

# Classification Tree

```{r}
set.seed(1)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
```

```{r}
tree <- rpart(Outcome ~., train.df, method = "class")
fancyRpartPlot(tree)
```

```{r}
tree <- rpart(Survived ~ ., train, method = "class", control = rpart.control(cp=0.00001))
# Draw the complex tree
fancyRpartPlot(tree)
# Prune the tree: pruned
pruned <- prune(tree, cp=0.01)
# Draw pruned
fancyRpartPlot(pruned)
```

```{r}
table(pred=tree.pred, true=test.df$Outcome)
printcp(tree)
# get index of CP with lowest xerror
tree.best <- which.min(tree$cptable[,"xerror"])
# get its value
cp <- tree$cptable[tree.best, "CP"]
cp
plotcp(tree)
```

```{r}
#prune tree
tree.pruned <- prune(tree,cp)
#plot tree
fancyRpartPlot(tree.pruned)
```

```{r}
tree_i <- rpart(spam ~ ., train, method = "class", parms = list(split = "information"))
pred_i <- predict(tree_i, test, type = "class")
conf_i <- table(test$spam, pred_i)
acc_i <- sum(diag(conf_i)) / sum(conf_i)
```

```{r}
tree.pred <- predict(tree, test.df, type="class")
tree.conf <- table(test.df$Outcome, tree.pred)
sum(diag(tree.conf))/sum(tree.conf)
```

```{r}
evaluation <- function(model, data, atype) {
  cat("\nConfusion matrix:\n")
  prediction = predict(model, data, type=atype)
  xtab = table(prediction, data$Outcome)
  print(xtab)
  cat("\nEvaluation:\n\n")
  accuracy = sum(prediction == data$Outcome)/length(data$Outcome)
  precision = xtab[1,1]/sum(xtab[,1])
  recall = xtab[1,1]/sum(xtab[1,])
  f = 2 * (precision * recall) / (precision + recall)
  cat(paste("Accuracy:\t", format(accuracy, digits=2), "\n",sep=" "))
  cat(paste("Precision:\t", format(precision, digits=2), "\n",sep=" "))
  cat(paste("Recall:\t\t", format(recall, digits=2), "\n",sep=" "))
  cat(paste("F-measure:\t", format(f, digits=2), "\n",sep=" "))
}
evaluation(tree, test.df, "class")
```

########################################################################################

### KNN

```{r}
set.seed(1)
randOrder = order(runif(nrow(diabetes)))
train.df = subset(diabetes,randOrder < .8 * nrow(diabetes))
test.df = subset(diabetes,randOrder > .8 * nrow(diabetes))

# add interaction terms
#train.df$agebmi <- train.df$BMI*train.df$Age
#train.df$agebl <- train.df$BloodPressure*train.df$Age
train.df$agegl <- train.df$Glucose*train.df$Age

#test.df$agebmi <- test.df$BMI*test.df$Age
#test.df$agebl <- test.df$BloodPressure*test.df$Age
test.df$agegl <- test.df$Glucose*test.df$Age

seg.flg.num <- model.matrix(~., data = train.df)
seg.flg.num <- seg.flg.num [,-1]

# scaling the data
scaled_data <- scale(seg.flg.num)
scaled_data <- as.data.frame(scaled_data) 

randOrder = order(runif(nrow(scaled_data)))
train.d = subset(train.df,randOrder < .8 * nrow(scaled_data))
test.d = subset(train.df,randOrder > .8 * nrow(scaled_data))

# initialize a data frame with two columns: k, and accuracy.
accuracy.df <- data.frame(k = seq(1, 14, 1), accuracy = rep(0, 14))
test.d$Outcome <- as.factor(test.d$Outcome)

# compute knn for different k on validation.
for(i in 1:14) {
  knn.pred <- knn(train.d,test.d, train.d$Outcome,k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, test.d$Outcome)$overall[1]
}
plot(accuracy.df)   # accuracy is highest when k = 9
accuracy.df
```









